{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from subsample_trainer import SGDSubsampleClassifier\n",
    "from subsample_trainer import CNBSubsampleClassifier\n",
    "from subsample_trainer import CNBTwoStepClassifier\n",
    "from subsample_trainer import SGDTwoStepClassifier\n",
    "from subsample_trainer import RFTwoStepClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import operator\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "def debug_msg(message):\n",
    "    if DEBUG:\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating data CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple CSV files were created, parsing the data in different ways to save time on future calls.  This code is included merely for completeness, and because the resulting CSVs were too large to include in the GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top10s file had a large amount of interesting data in terms of genre, popularity, beats per minute, etc; however, it didn't have the actual lyrics of the songs.  To get around this, we scraped the lyrics from metrolyrics.com, as did the creators of the other dataset.  I also condensed the genres to ensure each genre included more than 1 song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findw(w1, w2):\n",
    "    w1l = re.split(r\"\\W+\", w1)\n",
    "    return w2 in w1l\n",
    "\n",
    "def group_genre(top_genre):\n",
    "    top_genre = ' '+top_genre+' '\n",
    "    if findw(top_genre, 'hip'):\n",
    "        return 'rap'\n",
    "    elif findw(top_genre, 'pop'):\n",
    "        return 'pop'\n",
    "    elif findw(top_genre, 'soul'):\n",
    "        return 'r&b'\n",
    "    elif findw(top_genre, 'r') and findw(top_genre, 'b'):\n",
    "        return 'r&b'\n",
    "    elif findw(top_genre, 'boy'):\n",
    "        return 'boy band'\n",
    "    elif findw(top_genre, 'rap'):\n",
    "        return 'rap'\n",
    "    elif findw(top_genre, 'rock'):\n",
    "        return 'pop'\n",
    "    elif findw(top_genre, 'room'):\n",
    "        return 'house'\n",
    "    elif findw(top_genre, 'house'):\n",
    "        return 'house'\n",
    "    elif findw(top_genre, 'metropopolis'):\n",
    "        return 'pop'\n",
    "    elif findw(top_genre, 'indie'):\n",
    "        return 'pop'\n",
    "    elif findw(top_genre, 'singer'):\n",
    "        return 'r&b'\n",
    "    elif findw(top_genre, 'techno'):\n",
    "        return 'electronic'\n",
    "    elif findw(top_genre, 'edm'):\n",
    "        return 'electronic'\n",
    "    elif findw(top_genre, 'complextro'):\n",
    "        return 'electronic'\n",
    "    elif findw(top_genre, 'electro'):\n",
    "        return 'electronic'\n",
    "    elif findw(top_genre, 'electropop'):\n",
    "        return 'electronic'\n",
    "    elif findw(top_genre, 'wave'):\n",
    "        return 'alternative'\n",
    "    elif findw(top_genre, 'brostep'):\n",
    "        return 'electronic'\n",
    "    elif findw(top_genre, 'dubstep'):\n",
    "        return 'electronic'\n",
    "    elif findw(top_genre, 'dance'):\n",
    "        return 'pop'\n",
    "    elif findw(top_genre, 'latin'):\n",
    "        return 'latin'\n",
    "    elif findw(top_genre, 'hollywood'):\n",
    "        return 'pop'\n",
    "    elif findw(top_genre, 'electronic') and findw(top_genre, 'trap'):\n",
    "        return 'house'\n",
    "    elif findw(top_genre, 'country'):\n",
    "        return 'pop'\n",
    "    else:\n",
    "        return top_genre.strip()\n",
    "\n",
    "def infer_metrolyrics_url(title, artist):\n",
    "    delimiter_re = r\"(\\(.*\\))*\\W\"\n",
    "    url = 'https://www.metrolyrics.com/'\n",
    "    title = [w for w in re.split(delimiter_re, title.lower()) if w != '' and w is not None and w[0] != '(']\n",
    "    artist = [w for w in re.split(delimiter_re, artist.lower()) if w != '' and w is not None and w[0] != '(']\n",
    "    for word in title:\n",
    "        url += word + '-'\n",
    "    url += 'lyrics'\n",
    "    for word in artist:\n",
    "        url += '-' + word\n",
    "    url += '.html'\n",
    "    return url\n",
    "    \n",
    "\n",
    "song_dataset = pd.read_csv('top10s.csv')\n",
    "lyrics_dataset = {'Song': [],\n",
    "                  'Artist': [],\n",
    "                  'Year': [],\n",
    "                  'Genre-group': [],\n",
    "                  'Popularity': [],\n",
    "                  'Lyrics': []}\n",
    "\n",
    "counter = 0\n",
    "for index in range(0, len(song_dataset['title'])):\n",
    "    if index % 100 == 0:\n",
    "        debug_msg('Reading song #'+str(index)+'/'+str(len(song_dataset['title']))+': '+str(counter)+' successful')\n",
    "    title = song_dataset['title'][index]\n",
    "    artist = song_dataset['artist'][index]\n",
    "    top_genre = song_dataset['top genre'][index]\n",
    "    year = song_dataset['year'][index]\n",
    "    popularity = song_dataset['pop'][index]\n",
    "    URL = infer_metrolyrics_url(title, artist)\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    lyricbody = soup.find(id='lyrics-body')\n",
    "    if lyricbody is None: # Means that it pulled up a 404 page most likely\n",
    "        continue\n",
    "    verses_iterable = lyricbody.find_all('p', class_='verse')\n",
    "    lyrics = ''\n",
    "    for verse in verses_iterable:\n",
    "        lyrics += verse.text.strip() + ' '\n",
    "    lyrics_dataset['Song'].append(title)\n",
    "    lyrics_dataset['Artist'].append(artist)\n",
    "    lyrics_dataset['Year'].append(year)\n",
    "    lyrics_dataset['Genre-group'].append(group_genre(top_genre))\n",
    "    lyrics_dataset['Popularity'].append(popularity)\n",
    "    lyrics_dataset['Lyrics'].append(lyrics)\n",
    "    counter += 1\n",
    "\n",
    "debug_msg('Processing done! Saving data...')\n",
    "df = pd.DataFrame(lyrics_dataset)\n",
    "df.to_csv('lyrics_spotify.csv', encoding='utf-8')\n",
    "debug_msg('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the lyrics dataset for our purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"lyrics_spotify.csv\")\n",
    "    \n",
    "dirty_dict = {\"lyrics\" : [],\n",
    "                \"genre\" : [],\n",
    "                \"popularity\" : []}\n",
    "# Basic cleaning of impromper imports\n",
    "for index in range(0, len(data['Lyrics'])):\n",
    "    if index % 100 == 0:\n",
    "        debug_msg('Checking song #'+str(index)+'/'+str(len(data['Lyrics'])))\n",
    "    if(isinstance(data['Lyrics'][index], str)):\n",
    "        dirty_dict['lyrics'].append(data['Lyrics'][index])\n",
    "        dirty_dict['genre'].append(data['Genre-group'][index])\n",
    "        dirty_dict['popularity'].append(data['Popularity'][index])\n",
    "\n",
    "df_dirty = pd.DataFrame(dirty_dict)\n",
    "\n",
    "df_dirty.to_csv('dirty_lyrics_genre_popularity.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning stopwords and punctuation from lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_unnecessary_tokenization(wordlist):\n",
    "    newlist = []\n",
    "    i = 0\n",
    "    while i < len(wordlist)-1:\n",
    "        if wordlist[i] == 'got' and wordlist[i+1] == 'ta':\n",
    "            newlist.append('gotta')\n",
    "            i += 2\n",
    "        elif wordlist[i] == 'gon' and wordlist[i+1] == 'na':\n",
    "            newlist.append('gonna')\n",
    "            i += 2\n",
    "        elif wordlist[i] == 'wan' and wordlist[i+1] == 'na':\n",
    "            newlist.append('wanna')\n",
    "            i += 2\n",
    "        else:\n",
    "            newlist.append(wordlist[i])\n",
    "            i += 1\n",
    "    if i < len(wordlist):\n",
    "        newlist.append(wordlist[i])\n",
    "    return newlist\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"lyrics_spotify.csv\")\n",
    "columns = data.columns\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "stop_words_ext = set(stopwords.words('english') + list(string.punctuation) + ['yeah', 'oh', 'ohh', 'woah', 'la', 'na'])\n",
    "ps = PorterStemmer()\n",
    "# Create dummy lists to populate with values\n",
    "lyrics_tokens = []\n",
    "genre_targets = []\n",
    "popularity_targets = []\n",
    "\n",
    "genre_counts = {}\n",
    "genre_words = {}\n",
    "all_words = []\n",
    "\n",
    "# Break lyrics up into tokens and get genre counts\n",
    "for index in range(0, len(data['Lyrics'])):\n",
    "    if index % 100 == 0:\n",
    "        debug_msg('Tokenizing song #'+str(index)+'/'+str(len(data['Lyrics'])))\n",
    "    if data['Genre-group'][index] in genre_counts:\n",
    "        genre_counts[data['Genre-group'][index]] += 1\n",
    "    else:\n",
    "        genre_counts[data['Genre-group'][index]] = 1\n",
    "        genre_words[data['Genre-group'][index]] = []\n",
    "    if(isinstance(data['Lyrics'][index], str)):\n",
    "        lyrics = (data['Lyrics'][index]).translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "        lyrics_tokenized = [w.lower() for w in word_tokenize(lyrics) if not w in stop_words_ext] # clean stopwords\n",
    "        lyrics_tokenized = fix_unnecessary_tokenization(lyrics_tokenized)\n",
    "        lyrics_tokens.append(lyrics_tokenized) # Get song data per song\n",
    "        genre_words[data['Genre-group'][index]] += lyrics_tokenized\n",
    "        all_words += lyrics_tokenized\n",
    "        genre_targets.append(data['Genre-group'][index]) # Get targets\n",
    "        popularity_targets.append(data['Popularity'][index])\n",
    "unique_words = list(dict.fromkeys(all_words))\n",
    "debug_msg(len(lyrics_tokens))\n",
    "\n",
    "# Now treat words that occur fewer than 10 times across all classes as unknown words (disregard)\n",
    "known_words = []\n",
    "debug_msg('Removing outlier words')\n",
    "for index in range(0, len(unique_words)):\n",
    "    w = unique_words[index]\n",
    "    if index % 100 == 0:\n",
    "        debug_msg('Checking word #'+str(index)+'/'+str(len(unique_words)))\n",
    "    known = False\n",
    "    for g in genre_words:\n",
    "        if genre_words[g].count(w) >= 10:\n",
    "            known = True\n",
    "    if known:\n",
    "        known_words.append(w)\n",
    "    else:\n",
    "        for g in genre_words:\n",
    "            genre_words[g] = [r for r in genre_words[g] if r != w]\n",
    "        for i in range(0, len(lyrics_tokens)):\n",
    "            lyrics_tokens[i] = [r for r in lyrics_tokens[i] if r != w]\n",
    "debug_msg(len(lyrics_tokens))\n",
    "\n",
    "# Re-concatenate into cleaned lyrics for sklearn\n",
    "lyrics = ['']*len(lyrics_tokens)\n",
    "for i in range(0, len(lyrics_tokens)):\n",
    "    if i % 100 == 0:\n",
    "        debug_msg('Reforming song #'+str(i)+'/'+str(len(lyrics_tokens)))\n",
    "    for w in lyrics_tokens[i]:\n",
    "        lyrics[i] += w + ' '\n",
    "debug_msg(len(lyrics_tokens))\n",
    "\n",
    "total_data = {'lyrics' : lyrics, 'genre' : genre_targets, 'popularity' : popularity_targets}\n",
    "df = pd.DataFrame(total_data)\n",
    "df.to_csv('cleaned_scraped_lyrics_genre.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating trigrams from the raw lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"lyrics_spotify.csv\")\n",
    "columns = data.columns\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "stop_words_ext = set(stopwords.words('english') + list(string.punctuation) + ['yeah', 'oh', 'ohh', 'woah', 'la', 'na'])\n",
    "ps = PorterStemmer()\n",
    "# Create dummy lists to populate with values\n",
    "# lyrics_tokens = [None]*len(data['Lyrics'])\n",
    "\n",
    "song_dict = {'lyrics' : [],\n",
    "             'genre' : [],\n",
    "             'popularity' : []}\n",
    "\n",
    "SEP = ''\n",
    "\n",
    "# Break lyrics up into tokens\n",
    "for index in range(0, len(data['Lyrics'])):\n",
    "    if index % 100 == 0:\n",
    "        debug_msg('Tokenizing song #'+str(index)+'/'+str(len(data['Lyrics'])))\n",
    "    if(isinstance(data['Lyrics'][index], str)):\n",
    "        lyrics = (data['Lyrics'][index]).translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = [w.lower() for w in word_tokenize(lyrics) if not w in list(string.punctuation)]\n",
    "        tokens = fix_unnecessary_tokenization(tokens)\n",
    "        new_lyrics = ''\n",
    "        for i in range(len(tokens)-2):\n",
    "            trigram = tokens[i]+SEP+tokens[i+1]+SEP+tokens[i+2]\n",
    "            new_lyrics += trigram + ' '\n",
    "        song_dict['lyrics'].append(new_lyrics)\n",
    "        song_dict['genre'].append(data['Genre-group'][index])\n",
    "        song_dict['popularity'].append(data['Popularity'][index])\n",
    "\n",
    "debug_msg('Processing done! Saving data...')\n",
    "df = pd.DataFrame(song_dict)\n",
    "df.to_csv('trigram_lyrics.csv', encoding='utf-8')\n",
    "debug_msg('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Genre on Lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers are saved in object name, argument tuples to allow for multiple instantiations on new data.  Unfortunately, this has the downside of not allowing the automation of more specific calls, such as calling the RandomForestClassifier with a limited max_depth; however, these calls were done externally and were found to not improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_arg = {'CNB' : (ComplementNB, 1e-05), \n",
    "           'SVM' : (SGDClassifier, None), \n",
    "           'RF' : (RandomForestClassifier, 50),\n",
    "           'KNN' : (KNeighborsClassifier, 1),\n",
    "           'SGDSC' : (SGDSubsampleClassifier, None),\n",
    "           'CNBSC' : (CNBSubsampleClassifier, None),\n",
    "           'CNB2C' : (CNBTwoStepClassifier, 0.001),\n",
    "           'SVM2C' : (SGDTwoStepClassifier, 0.001),\n",
    "           'RF2C' : (RFTwoStepClassifier, None)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_res(df, bigram=True):\n",
    "    res = {}\n",
    "    for name in clf_arg:\n",
    "        res['n-p train '+name] = []\n",
    "        res['n-p test  '+name] = []\n",
    "        res['pop train '+name] = []\n",
    "        res['pop test  '+name] = []\n",
    "        res['all train '+name] = []\n",
    "        res['all test  '+name] = []\n",
    "        \n",
    "    num_iters = 10\n",
    "    for iteration in range(num_iters):\n",
    "        debug_msg('Iteration '+str(iteration)+'/'+str(num_iters))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df['lyrics'], df['genre'], test_size=0.3) # random_state=0\n",
    "        X_train_n, y_train_n = list(zip(*[(s,g) for s,g in list(zip(X_train,y_train)) if g != 'pop']))\n",
    "        X_test_n, y_test_n = list(zip(*[(s,g) for s,g in list(zip(X_test,y_test)) if g != 'pop']))\n",
    "        X_train_p, y_train_p = list(zip(*[(s,g) for s,g in list(zip(X_train,y_train)) if g == 'pop']))\n",
    "        X_test_p, y_test_p = list(zip(*[(s,g) for s,g in list(zip(X_test,y_test)) if g == 'pop']))\n",
    "        \n",
    "        for name in clf_arg:\n",
    "            debug_msg(name)\n",
    "            clf, arg = clf_arg[name]\n",
    "            if arg != None:\n",
    "                clf_init = clf(arg)\n",
    "            else:\n",
    "                clf_init = clf()\n",
    "            if bigram:\n",
    "                pipeline = Pipeline([\n",
    "                    ('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ('clf', clf_init),\n",
    "                    ])\n",
    "            else:\n",
    "                pipeline = Pipeline([\n",
    "                    ('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ('clf', clf_init),\n",
    "                    ])\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            res['n-p train '+name].append(pipeline.score(X_train_n, y_train_n))\n",
    "            res['n-p test  '+name].append(pipeline.score(X_test_n, y_test_n))\n",
    "            res['pop train '+name].append(pipeline.score(X_train_p, y_train_p))\n",
    "            res['pop test  '+name].append(pipeline.score(X_test_p, y_test_p))\n",
    "            res['all train '+name].append(pipeline.score(X_train, y_train))\n",
    "            res['all test  '+name].append(pipeline.score(X_test, y_test))\n",
    "    avg_scores = {\n",
    "        'which genres' : [],\n",
    "        'train or test' : [],\n",
    "        'classifier' : [],\n",
    "        'score' : []\n",
    "    }\n",
    "    for test in res:\n",
    "        test_designators = [s for s in test.split(' ') if s != '']\n",
    "        score = np.mean(res[test])\n",
    "        avg_scores['which genres'].append(test_designators[0])\n",
    "        avg_scores['train or test'].append(test_designators[1])\n",
    "        avg_scores['classifier'].append(test_designators[2])\n",
    "        avg_scores['score'].append(score)\n",
    "    \n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_options = {\n",
    "    'RAW LYRICS' : 'dirty_lyrics_genre_popularity.csv',\n",
    "    'CLEANED LYRICS' : 'cleaned_scraped_lyrics_genre.csv',\n",
    "    \n",
    "} # 'TRIGRAMS' : 'trigram_lyrics.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for dataset in data_options:\n",
    "    path = data_options[dataset]\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    avg_scores = pd.DataFrame(avg_res(df))\n",
    "    scores.append(avg_scores)\n",
    "    print('>>> ' + dataset)\n",
    "    print(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, dataset in list(zip(scores, data_options)):\n",
    "    df.to_csv(dataset+'_genre_predictions.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores = pd.DataFrame(avg_res(pd.read_csv('trigram_lyrics.csv'), False))\n",
    "avg_scores.to_csv('TRIGRAMS_genre_predictions.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
